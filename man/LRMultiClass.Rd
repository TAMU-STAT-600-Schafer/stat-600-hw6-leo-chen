% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/LR_wrapper.R
\name{LRMultiClass}
\alias{LRMultiClass}
\title{Multiclass Logistic Regression with Ridge Regularization}
\usage{
LRMultiClass(X, y, numIter = 100, eta = 0.1, lambda = 1, beta_init = NULL)
}
\arguments{
\item{X}{A numeric matrix of predictors where the first column must be all 1s (intercept term).
Each row represents an observation and each column represents a feature.}

\item{y}{A numeric vector of class labels. If classes start from 1, they will be
automatically converted to 0-based indexing (0 to K-1).}

\item{numIter}{An integer specifying the number of iterations for Newton's method.
Default is 100.}

\item{eta}{A positive number specifying the damping parameter for Newton's method.
Controls the step size of parameter updates. Default is 0.1.}

\item{lambda}{A non-negative number specifying the ridge regularization parameter.
Larger values result in stronger regularization. Default is 1.}

\item{beta_init}{Optional initial values for the coefficient matrix. If NULL (default),
initializes with a matrix of zeros of size p × K, where p is the
number of features and K is the number of classes.}
}
\value{
A list containing:
        \item{beta}{The final coefficient matrix (p × K) where p is the number
              of features (including intercept) and K is the number of classes}
        \item{objective}{A numeric vector containing the objective function values
              at each iteration, showing the convergence behavior}
}
\description{
Implements multiclass logistic regression using damped Newton's method with ridge regularization.
The implementation uses C++ for efficient computation and handles multiple classes through
a one-vs-all approach.
}
\details{
The function implements multinomial logistic regression using damped Newton's
method for optimization. The objective function includes a ridge penalty term
(λ/2)||β||²) for regularization. The algorithm uses a C++ implementation for
efficient computation of gradients and Hessians.
}
\examples{
# Generate example data
set.seed(123)
n <- 50  # number of observations
p <- 2   # number of features (excluding intercept)
K <- 3   # number of classes

# Create feature matrix with intercept
X <- cbind(1, matrix(rnorm(n * p), n, p))

# Generate class labels (0, 1, 2)
y <- sample(0:(K-1), n, replace = TRUE)

# Fit the model
result <- LRMultiClass(X, y, numIter = 50)

# Examine results
head(result$beta)  # View coefficient matrix
plot(result$objective, type = "l",
     xlab = "Iteration", ylab = "Objective value",
     main = "Convergence plot")

}
